{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31500 reading disorder', \"the essential feature of reading disorder is reading achievement reading accuracy speed or comprehension as measured by individually administered standardized tests that falls substantially below that expected given the individual's chronological age measured intelligence and age appropriate education criterion a the disturbance in reading significantly interferes with academic achievement or with activities of daily living that require reading skills criterion b if a sensory deficit is present the reading difficulties are in excess of those usually associated with it criterion c if a neurological or other general medical condition or sensory deficit is present it should be coded on axis iii in individuals with reading disorder which has also been called dyslexia oral reading is characterized by distortions substitutions or omissions both oral and silent reading are characterized by slowness and errors in comprehension\", '3151 mathematics disorder']\n",
      "[[u'reading', u'disorder'], [u'essential', u'feature', u'reading', u'disorder', u'reading', u'achievement', u'reading', u'accuracy', u'speed', u'comprehension', u'measured', u'individually', u'administered', u'standardized', u'tests', u'falls', u'substantially', u'expected', u'given', u'chronological', u'age', u'measured', u'intelligence', u'age', u'appropriate', u'education', u'criterion', u'disturbance', u'reading', u'significantly', u'interferes', u'academic', u'achievement', u'activities', u'daily', u'living', u'require', u'reading', u'skills', u'criterion', u'b', u'sensory', u'deficit', u'present', u'reading', u'difficulties', u'excess', u'usually', u'associated', u'criterion', u'c', u'neurological', u'general', u'medical', u'condition', u'sensory', u'deficit', u'present', u'coded', u'axis', u'iii', u'individuals', u'reading', u'disorder', u'also', u'called', u'dyslexia', u'oral', u'reading', u'characterized', u'distortions', u'substitutions', u'omissions', u'oral', u'silent', u'reading', u'characterized', u'slowness', u'errors', u'comprehension'], [u'mathematics', u'disorder'], [u'essential', u'feature', u'mathematics', u'disorder', u'mathematical', u'ability', u'measured', u'individually', u'administered', u'standardized', u'tests', u'mathematical', u'calculation', u'reasoning', u'falls', u'substantially', u'expected', u'chronological', u'age', u'measured', u'intelligence', u'age', u'appropriate', u'education', u'criterion', u'disturbance', u'mathematics', u'significantly', u'interferes', u'academic', u'achievement', u'activities', u'daily', u'living', u'require', u'mathematical', u'skills', u'criterion', u'b', u'sensory', u'deficit', u'present', u'difficulties', u'mathematical', u'ability', u'excess', u'usually', u'associated', u'criterion', u'c', u'neurological', u'general', u'medical', u'condition', u'sensory', u'deficit', u'present', u'coded', u'axis', u'iii', u'number', u'different', u'skills', u'may', u'impaired', u'mathematics', u'disorder', u'including', u'linguistic', u'skills', u'understanding', u'naming', u'mathematical', u'terms', u'operations', u'concepts', u'decoding', u'written', u'problems', u'mathematical', u'symbols', u'perceptual', u'skills', u'recognizing', u'reading', u'numerical', u'symbols', u'arithmetic', u'signs', u'clustering', u'objects', u'groups', u'attention', u'skills', u'copying', u'numbers', u'figures', u'correctly', u'remembering', u'add', u'carried', u'numbers', u'observing', u'operational', u'signs', u'mathematical', u'skills', u'following', u'sequences', u'mathematical', u'steps', u'counting', u'objects', u'learning', u'multiplication', u'tables'], [u'disorder', u'written', u'expression']]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    print (list)\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace():\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    print (DSM4.take(3))\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w))\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31500 reading disorder', '', \"the essential feature of reading disorder is reading achievement reading accuracy speed or comprehension as measured by individually administered standardized tests that falls substantially below that expected given the individual's chronological age measured intelligence and age appropriate education criterion a the disturbance in reading significantly interferes with academic achievement or with activities of daily living that require reading skills criterion b if a sensory deficit is present the reading difficulties are in excess of those usually associated with it criterion c if a neurological or other general medical condition or sensory deficit is present it should be coded on axis iii in individuals with reading disorder which has also been called dyslexia oral reading is characterized by distortions substitutions or omissions both oral and silent reading are characterized by slowness and errors in comprehension\"]\n",
      "[[u'reading', u'disorder'], [], [u'essential', u'feature', u'reading', u'disorder', u'reading', u'achievement', u'reading', u'accuracy', u'speed', u'comprehension', u'measured', u'individually', u'administered', u'standardized', u'tests', u'falls', u'substantially', u'expected', u'given', u'chronological', u'age', u'measured', u'intelligence', u'age', u'appropriate', u'education', u'criterion', u'disturbance', u'reading', u'significantly', u'interferes', u'academic', u'achievement', u'activities', u'daily', u'living', u'require', u'reading', u'skills', u'criterion', u'b', u'sensory', u'deficit', u'present', u'reading', u'difficulties', u'excess', u'usually', u'associated', u'criterion', u'c', u'neurological', u'general', u'medical', u'condition', u'sensory', u'deficit', u'present', u'coded', u'axis', u'iii', u'individuals', u'reading', u'disorder', u'also', u'called', u'dyslexia', u'oral', u'reading', u'characterized', u'distortions', u'substitutions', u'omissions', u'oral', u'silent', u'reading', u'characterized', u'slowness', u'errors', u'comprehension'], [], [u'mathematics', u'disorder']]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == ' '):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    print (DSM4.take(3))\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w))\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31500 reading disorder', '', \"the essential feature of reading disorder is reading achievement reading accuracy speed or comprehension as measured by individually administered standardized tests that falls substantially below that expected given the individual's chronological age measured intelligence and age appropriate education criterion a the disturbance in reading significantly interferes with academic achievement or with activities of daily living that require reading skills criterion b if a sensory deficit is present the reading difficulties are in excess of those usually associated with it criterion c if a neurological or other general medical condition or sensory deficit is present it should be coded on axis iii in individuals with reading disorder which has also been called dyslexia oral reading is characterized by distortions substitutions or omissions both oral and silent reading are characterized by slowness and errors in comprehension\"]\n",
      "[[u'reading', u'disorder'], [], [u'essential', u'feature', u'reading', u'disorder', u'reading', u'achievement', u'reading', u'accuracy', u'speed', u'comprehension', u'measured', u'individually', u'administered', u'standardized', u'tests', u'falls', u'substantially', u'expected', u'given', u'chronological', u'age', u'measured', u'intelligence', u'age', u'appropriate', u'education', u'criterion', u'disturbance', u'reading', u'significantly', u'interferes', u'academic', u'achievement', u'activities', u'daily', u'living', u'require', u'reading', u'skills', u'criterion', u'b', u'sensory', u'deficit', u'present', u'reading', u'difficulties', u'excess', u'usually', u'associated', u'criterion', u'c', u'neurological', u'general', u'medical', u'condition', u'sensory', u'deficit', u'present', u'coded', u'axis', u'iii', u'individuals', u'reading', u'disorder', u'also', u'called', u'dyslexia', u'oral', u'reading', u'characterized', u'distortions', u'substitutions', u'omissions', u'oral', u'silent', u'reading', u'characterized', u'slowness', u'errors', u'comprehension'], [], [u'mathematics', u'disorder']]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == ('\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    print (DSM4.take(3))\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w))\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-4454593d68bd>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-4454593d68bd>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == ('\\n'):\u001b[0m\n\u001b[0m                                                                                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == ('\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    print (DSM4.take(3))\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w))\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31500 reading disorder', '', \"the essential feature of reading disorder is reading achievement reading accuracy speed or comprehension as measured by individually administered standardized tests that falls substantially below that expected given the individual's chronological age measured intelligence and age appropriate education criterion a the disturbance in reading significantly interferes with academic achievement or with activities of daily living that require reading skills criterion b if a sensory deficit is present the reading difficulties are in excess of those usually associated with it criterion c if a neurological or other general medical condition or sensory deficit is present it should be coded on axis iii in individuals with reading disorder which has also been called dyslexia oral reading is characterized by distortions substitutions or omissions both oral and silent reading are characterized by slowness and errors in comprehension\"]\n",
      "[[u'reading', u'disorder'], [], [u'essential', u'feature', u'reading', u'disorder', u'reading', u'achievement', u'reading', u'accuracy', u'speed', u'comprehension', u'measured', u'individually', u'administered', u'standardized', u'tests', u'falls', u'substantially', u'expected', u'given', u'chronological', u'age', u'measured', u'intelligence', u'age', u'appropriate', u'education', u'criterion', u'disturbance', u'reading', u'significantly', u'interferes', u'academic', u'achievement', u'activities', u'daily', u'living', u'require', u'reading', u'skills', u'criterion', u'b', u'sensory', u'deficit', u'present', u'reading', u'difficulties', u'excess', u'usually', u'associated', u'criterion', u'c', u'neurological', u'general', u'medical', u'condition', u'sensory', u'deficit', u'present', u'coded', u'axis', u'iii', u'individuals', u'reading', u'disorder', u'also', u'called', u'dyslexia', u'oral', u'reading', u'characterized', u'distortions', u'substitutions', u'omissions', u'oral', u'silent', u'reading', u'characterized', u'slowness', u'errors', u'comprehension'], [], [u'mathematics', u'disorder']]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    print (DSM4.take(3))\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w).flatMap(lambda h: h))\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31500 reading disorder', '', \"the essential feature of reading disorder is reading achievement reading accuracy speed or comprehension as measured by individually administered standardized tests that falls substantially below that expected given the individual's chronological age measured intelligence and age appropriate education criterion a the disturbance in reading significantly interferes with academic achievement or with activities of daily living that require reading skills criterion b if a sensory deficit is present the reading difficulties are in excess of those usually associated with it criterion c if a neurological or other general medical condition or sensory deficit is present it should be coded on axis iii in individuals with reading disorder which has also been called dyslexia oral reading is characterized by distortions substitutions or omissions both oral and silent reading are characterized by slowness and errors in comprehension\"]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 6.0 failed 1 times, most recent failure: Lost task 4.0 in stage 6.0 (TID 65, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1295, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-6-129fe3c615aa>\", line 41, in <lambda>\nAttributeError: 'list' object has no attribute 'flatMap'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1839)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:361)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1295, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-6-129fe3c615aa>\", line 41, in <lambda>\nAttributeError: 'list' object has no attribute 'flatMap'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-129fe3c615aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDSM4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 6.0 failed 1 times, most recent failure: Lost task 4.0 in stage 6.0 (TID 65, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1295, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-6-129fe3c615aa>\", line 41, in <lambda>\nAttributeError: 'list' object has no attribute 'flatMap'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1839)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:361)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1295, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-6-129fe3c615aa>\", line 41, in <lambda>\nAttributeError: 'list' object has no attribute 'flatMap'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    print (DSM4.take(3))\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).flatMap(lambda h: h)\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-6-129fe3c615aa>:32 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6a782a3f9709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Open / read file and load and parse data into sections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    248\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 250\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-6-129fe3c615aa>:32 "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    print (DSM4.take(3))\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).flatMap(lambda h: h)\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-6-129fe3c615aa>:32 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6a782a3f9709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Open / read file and load and parse data into sections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    248\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 250\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-6-129fe3c615aa>:32 "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    print (DSM4.take(3))\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).flatMap(lambda h: h)\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-6-129fe3c615aa>:32 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-513416a4ebfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Open / read file and load and parse data into sections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    248\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 250\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-6-129fe3c615aa>:32 "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    print (DSM4.take(3))\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.flatMap(lambda w: clean(w)).collect()\n",
    "\n",
    "    print(cleaned)\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-4454593d68bd>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-4454593d68bd>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == ('\\n'):\u001b[0m\n\u001b[0m                                                                                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    print (DSM4.take(3))\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w))\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31500 reading disorder', '', \"the essential feature of reading disorder is reading achievement reading accuracy speed or comprehension as measured by individually administered standardized tests that falls substantially below that expected given the individual's chronological age measured intelligence and age appropriate education criterion a the disturbance in reading significantly interferes with academic achievement or with activities of daily living that require reading skills criterion b if a sensory deficit is present the reading difficulties are in excess of those usually associated with it criterion c if a neurological or other general medical condition or sensory deficit is present it should be coded on axis iii in individuals with reading disorder which has also been called dyslexia oral reading is characterized by distortions substitutions or omissions both oral and silent reading are characterized by slowness and errors in comprehension\"]\n",
      "[[u'reading', u'disorder'], [], [u'essential', u'feature', u'reading', u'disorder', u'reading', u'achievement', u'reading', u'accuracy', u'speed', u'comprehension', u'measured', u'individually', u'administered', u'standardized', u'tests', u'falls', u'substantially', u'expected', u'given', u'chronological', u'age', u'measured', u'intelligence', u'age', u'appropriate', u'education', u'criterion', u'disturbance', u'reading', u'significantly', u'interferes', u'academic', u'achievement', u'activities', u'daily', u'living', u'require', u'reading', u'skills', u'criterion', u'b', u'sensory', u'deficit', u'present', u'reading', u'difficulties', u'excess', u'usually', u'associated', u'criterion', u'c', u'neurological', u'general', u'medical', u'condition', u'sensory', u'deficit', u'present', u'coded', u'axis', u'iii', u'individuals', u'reading', u'disorder', u'also', u'called', u'dyslexia', u'oral', u'reading', u'characterized', u'distortions', u'substitutions', u'omissions', u'oral', u'silent', u'reading', u'characterized', u'slowness', u'errors', u'comprehension'], [], [u'mathematics', u'disorder']]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    print (DSM4.take(3))\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w))\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31500 reading disorder', '', \"the essential feature of reading disorder is reading achievement reading accuracy speed or comprehension as measured by individually administered standardized tests that falls substantially below that expected given the individual's chronological age measured intelligence and age appropriate education criterion a the disturbance in reading significantly interferes with academic achievement or with activities of daily living that require reading skills criterion b if a sensory deficit is present the reading difficulties are in excess of those usually associated with it criterion c if a neurological or other general medical condition or sensory deficit is present it should be coded on axis iii in individuals with reading disorder which has also been called dyslexia oral reading is characterized by distortions substitutions or omissions both oral and silent reading are characterized by slowness and errors in comprehension\"]\n",
      "[['reading', 'disorder'], [], ['essential', 'feature', 'reading', 'disorder', 'reading', 'achievement', 'reading', 'accuracy', 'speed', 'comprehension', 'measured', 'individually', 'administered', 'standardized', 'tests', 'falls', 'substantially', 'expected', 'given', 'chronological', 'age', 'measured', 'intelligence', 'age', 'appropriate', 'education', 'criterion', 'disturbance', 'reading', 'significantly', 'interferes', 'academic', 'achievement', 'activities', 'daily', 'living', 'require', 'reading', 'skills', 'criterion', 'b', 'sensory', 'deficit', 'present', 'reading', 'difficulties', 'excess', 'usually', 'associated', 'criterion', 'c', 'neurological', 'general', 'medical', 'condition', 'sensory', 'deficit', 'present', 'coded', 'axis', 'iii', 'individuals', 'reading', 'disorder', 'also', 'called', 'dyslexia', 'oral', 'reading', 'characterized', 'distortions', 'substitutions', 'omissions', 'oral', 'silent', 'reading', 'characterized', 'slowness', 'errors', 'comprehension'], [], ['mathematics', 'disorder']]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower() #.decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    print (DSM4.take(3))\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w))\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['31500 reading disorder', '', \"the essential feature of reading disorder is reading achievement reading accuracy speed or comprehension as measured by individually administered standardized tests that falls substantially below that expected given the individual's chronological age measured intelligence and age appropriate education criterion a the disturbance in reading significantly interferes with academic achievement or with activities of daily living that require reading skills criterion b if a sensory deficit is present the reading difficulties are in excess of those usually associated with it criterion c if a neurological or other general medical condition or sensory deficit is present it should be coded on axis iii in individuals with reading disorder which has also been called dyslexia oral reading is characterized by distortions substitutions or omissions both oral and silent reading are characterized by slowness and errors in comprehension\"]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'take'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c48604d6b843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDSM4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'take'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower()#.decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    print (DSM4.take(3))\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).flatMap(lambda h: h).collect()\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-14-c48604d6b843>:32 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6e8a728416cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Open / read file and load and parse data into sections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    248\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 250\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-14-c48604d6b843>:32 "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).flatMap(lambda h: h)\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'reading', u'disorder', u'essential', u'feature', u'reading']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).flatMap(lambda h: h)\n",
    "\n",
    "    print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).flatMap(lambda h: h).collect()\n",
    "\n",
    "    #print(cleaned.take(5))\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:43\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).flatMap(lambda h: h)\n",
    "\n",
    "    print (cleaned)\n",
    "\n",
    "                                                          \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-8ff61e31ebbd>, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-8ff61e31ebbd>\"\u001b[0;36m, line \u001b[0;32m43\u001b[0m\n\u001b[0;31m    print a\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).collect()\n",
    "                                                          \n",
    "    sc.stop()\n",
    "    \n",
    "    for sent in cleaned:\n",
    "        print sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-fd3766a1e945>, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-fd3766a1e945>\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    print sent\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).collect()\n",
    "                                                          \n",
    "    sc.stop()\n",
    "    \n",
    "    #for sent in cleaned:\n",
    "    print cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-44600ba5894d>, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-44600ba5894d>\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    print cleaned\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).collect()\n",
    "        \n",
    "    #for sent in cleaned:\n",
    "    print cleaned\n",
    "    \n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-050df5ad6b50>, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-050df5ad6b50>\"\u001b[0;36m, line \u001b[0;32m42\u001b[0m\n\u001b[0;31m    print \"Hello.\"\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).collect()\n",
    "        \n",
    "    print \"Hello.\"\n",
    "    \n",
    "    sc.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-03655f5b767f>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-03655f5b767f>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    print \"Hello.\"\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    print ('Hello.')\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).collect()\n",
    "        \n",
    "\n",
    "    \n",
    "    sc.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    print ('Hello.')\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).collect()\n",
    "        \n",
    "\n",
    "    \n",
    "    sc.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello [[u'reading', u'disorder'], [], [u'essential', u'feature', u'reading', u'disorder', u'reading', u'achievement', u'reading', u'accuracy', u'speed', u'comprehension', u'measured', u'individually', u'administered', u'standardized', u'tests', u'falls', u'substantially', u'expected', u'given', u'chronological', u'age', u'measured', u'intelligence', u'age', u'appropriate', u'education', u'criterion', u'disturbance', u'reading', u'significantly', u'interferes', u'academic', u'achievement', u'activities', u'daily', u'living', u'require', u'reading', u'skills', u'criterion', u'b', u'sensory', u'deficit', u'present', u'reading', u'difficulties', u'excess', u'usually', u'associated', u'criterion', u'c', u'neurological', u'general', u'medical', u'condition', u'sensory', u'deficit', u'present', u'coded', u'axis', u'iii', u'individuals', u'reading', u'disorder', u'also', u'called', u'dyslexia', u'oral', u'reading', u'characterized', u'distortions', u'substitutions', u'omissions', u'oral', u'silent', u'reading', u'characterized', u'slowness', u'errors', u'comprehension'], [], [u'mathematics', u'disorder'], [], [u'essential', u'feature', u'mathematics', u'disorder', u'mathematical', u'ability', u'measured', u'individually', u'administered', u'standardized', u'tests', u'mathematical', u'calculation', u'reasoning', u'falls', u'substantially', u'expected', u'chronological', u'age', u'measured', u'intelligence', u'age', u'appropriate', u'education', u'criterion', u'disturbance', u'mathematics', u'significantly', u'interferes', u'academic', u'achievement', u'activities', u'daily', u'living', u'require', u'mathematical', u'skills', u'criterion', u'b', u'sensory', u'deficit', u'present', u'difficulties', u'mathematical', u'ability', u'excess', u'usually', u'associated', u'criterion', u'c', u'neurological', u'general', u'medical', u'condition', u'sensory', u'deficit', u'present', u'coded', u'axis', u'iii', u'number', u'different', u'skills', u'may', u'impaired', u'mathematics', u'disorder', u'including', u'linguistic', u'skills', u'understanding', u'naming', u'mathematical', u'terms', u'operations', u'concepts', u'decoding', u'written', u'problems', u'mathematical', u'symbols', u'perceptual', u'skills', u'recognizing', u'reading', u'numerical', u'symbols', u'arithmetic', u'signs', u'clustering', u'objects', u'groups', u'attention', u'skills', u'copying', u'numbers', u'figures', u'correctly', u'remembering', u'add', u'carried', u'numbers', u'observing', u'operational', u'signs', u'mathematical', u'skills', u'following', u'sequences', u'mathematical', u'steps', u'counting', u'objects', u'learning', u'multiplication', u'tables'], [], [u'disorder', u'written', u'expression'], [], [u'essential', u'feature', u'disorder', u'written', u'expression', u'writing', u'skills', u'measured', u'individually', u'administered', u'standardized', u'test', u'functional', u'assessment', u'writing', u'skills', u'fall', u'substantially', u'expected', u'given', u'chronological', u'age', u'measured', u'intelligence', u'age', u'appropriate', u'education', u'criterion', u'disturbance', u'written', u'expression', u'significantly', u'interferes', u'academic', u'achievement', u'activities', u'daily', u'living', u'require', u'writing', u'skills', u'criterion', u'b', u'sensory', u'deficit', u'present', u'difficulties', u'writing', u'skills', u'excess', u'usually', u'associated', u'criterion', u'c', u'neurological', u'general', u'medical', u'condition', u'sensory', u'deficit', u'present', u'coded', u'axis', u'iii', u'generally', u'combination', u'difficulties', u'ability', u'compose', u'written', u'texts', u'evidenced', u'grammatical', u'punctuation', u'errors', u'within', u'sentences', u'poor', u'paragraph', u'organization', u'multiple', u'spelling', u'errors', u'excessively', u'poor', u'handwriting', u'diagnosis', u'generally', u'given', u'spelling', u'errors', u'poor', u'handwriting', u'absence', u'impairment', u'written', u'expression', u'compared', u'learning', u'disorders', u'relatively', u'less', u'known', u'disorders', u'written', u'expression', u'remediation', u'particularly', u'occur', u'absence', u'reading', u'disorder', u'except', u'spelling', u'standardized', u'tests', u'area', u'less', u'well', u'developed', u'tests', u'reading', u'mathematical', u'ability', u'evaluation', u'impairment', u'written', u'skills', u'may', u'require', u'comparison', u'extensive', u'samples', u'written', u'schoolwork', u'expected', u'performance', u'age', u'iq', u'especially', u'case', u'young', u'children', u'early', u'elementary', u'grades', u'tasks', u'child', u'asked', u'copy', u'write', u'dictation', u'write', u'spontaneously', u'may', u'necessary', u'establish', u'presence', u'extent', u'disorder']]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower().decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    # contents = open(\"dsmiv.txt\").read()\n",
    "    contents = open(\"short.txt\").read()\n",
    "    #DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    cleaned = DSM4.map(lambda w: clean(w)).collect()\n",
    "    sc.stop()\n",
    "    \n",
    "    print (cleaned[2])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reading disorder', '']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower() #.decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    contents = open(\"short.txt\").read()\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    doc = DSM4.map(lambda w: ' '.join(clean(w))) #.collect()\n",
    "    #tdm.add_doc(doc)\n",
    "    print (doc.take(2))\n",
    "    sc.stop()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reading disorder', '', 'essential feature reading disorder reading achievement reading accuracy speed comprehension measured individually administered standardized tests falls substantially expected given chronological age measured intelligence age appropriate education criterion disturbance reading significantly interferes academic achievement activities daily living require reading skills criterion b sensory deficit present reading difficulties excess usually associated criterion c neurological general medical condition sensory deficit present coded axis iii individuals reading disorder also called dyslexia oral reading characterized distortions substitutions omissions oral silent reading characterized slowness errors comprehension']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower() #.decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and len(token)>3 and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    contents = open(\"short.txt\").read()\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    doc = DSM4.map(lambda w: ' '.join(clean(w))).collect()\n",
    "    for a in doc:\n",
    "        if len(doc) > 0:\n",
    "            tdm.add_doc(a.split())\n",
    "    print (doc.take(3))\n",
    "    sc.stop()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ebc03319e7e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tdm' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower() #.decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and len(token)>3 and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    contents = open(\"short.txt\").read()\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    doc = DSM4.map(lambda w: ' '.join(clean(w))).collect()\n",
    "    for a in doc:\n",
    "        if len(doc) > 0:\n",
    "            tdm.add_doc(a.split())\n",
    "    print (doc.take(3))\n",
    "    sc.stop()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-30-ebc03319e7e8>:32 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7d740a0c4cb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Open / read file and load and parse data into sections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    248\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 250\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-30-ebc03319e7e8>:32 "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower() #.decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and len(token)>3 and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    contents = open(\"short.txt\").read()\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    doc = DSM4.map(lambda w: ' '.join(clean(w))).collect()\n",
    "    #for a in doc:\n",
    "        #if len(doc) > 0:\n",
    "            #tdm.add_doc(a.split())\n",
    "    print (doc.take(3))\n",
    "    sc.stop()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-ebc03319e7e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tdm' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower() #.decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and len(token)>3 and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    contents = open(\"short.txt\").read()\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    doc = DSM4.map(lambda w: ' '.join(clean(w))).collect()\n",
    "    for a in doc:\n",
    "        if len(doc) > 0:\n",
    "            tdm.add_doc(a.split())\n",
    "    print (doc.take(3))\n",
    "    sc.stop()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'take'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2fd467726705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#if len(doc) > 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m#tdm.add_doc(a.split())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'take'"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower() #.decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and len(token)>3 and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    contents = open(\"short.txt\").read()\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    doc = DSM4.map(lambda w: ' '.join(clean(w))).collect()\n",
    "    #for a in doc:\n",
    "        #if len(doc) > 0:\n",
    "            #tdm.add_doc(a.split())\n",
    "    print (doc.take(3))\n",
    "    sc.stop()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reading disorder', '', 'essential feature reading disorder reading achievement reading accuracy speed comprehension measured individually administered standardized tests falls substantially expected given chronological measured intelligence appropriate education criterion disturbance reading significantly interferes academic achievement activities daily living require reading skills criterion sensory deficit present reading difficulties excess usually associated criterion neurological general medical condition sensory deficit present coded axis individuals reading disorder also called dyslexia oral reading characterized distortions substitutions omissions oral silent reading characterized slowness errors comprehension']\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower() #.decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and len(token)>3 and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sc = SparkContext()\n",
    "    # Open / read file and load and parse data into sections\n",
    "    delimiter = \"\\n\"\n",
    "    contents = open(\"short.txt\").read()\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    #the rdd of words to use\n",
    "    doc = DSM4.map(lambda w: ' '.join(clean(w)))#.collect()\n",
    "    #for a in doc:\n",
    "        #if len(doc) > 0:\n",
    "            #tdm.add_doc(a.split())\n",
    "    print (doc.take(3))\n",
    "    sc.stop()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower() #.decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and len(token)>3 and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize class to create term-document matrix\n",
    "    tdm = textmining.TermDocumentMatrix()\n",
    "    sc = SparkContext()\n",
    "    delimiter = \"\\n\"\n",
    "    \n",
    "    # Open / read file and load and parse data into sections\n",
    "    contents = open(\"short.txt\").read()\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    doc = DSM4.map(lambda w: ' '.join(clean(w))).map(lambda h: tdm.add_doc(h) if len(h) > 2 else None)\n",
    "    sc.stop()\n",
    "\n",
    "    tdm.write_csv('matrix2.csv', cutoff=1)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "from numpy import array\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "import textmining\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#filters out unwanted words and returns a list of words to use\n",
    "def clean(list):\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "    toret = []\n",
    "    for word in list.split():\n",
    "        token = word.lower() #.decode(\"ascii\", \"ignore\")\n",
    "        # remove stop words from tokens\n",
    "        if not token in en_stop and len(token)>3 and token.isalpha() and not token.isdigit() and not token.isspace() and not (token == '') and not (token == '\\n'):\n",
    "            toret.append(token)\n",
    "\n",
    "    return toret\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize class to create term-document matrix\n",
    "    tdm = textmining.TermDocumentMatrix()\n",
    "    sc = SparkContext()\n",
    "    delimiter = \"\\n\"\n",
    "    \n",
    "    # Open / read file and load and parse data into sections\n",
    "    contents = open(\"short.txt\").read()\n",
    "    DSM4 = sc.parallelize(contents.split(delimiter), 100)\n",
    "    doc = DSM4.map(lambda w: ' '.join(clean(w))).map(lambda h: h if len(h) > 2 else None).collect()\n",
    "    sc.stop()\n",
    "    \n",
    "    for d in doc:\n",
    "        if not d == None:\n",
    "            tdm.add_doc(d)\n",
    "    tdm.write_csv('matrix2.csv', cutoff=1)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
