{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Dec  9 19:50:08 2015\n",
    "\n",
    "@author: sdeppen\n",
    "\n",
    "sources used\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html#importing-your-documents\n",
    "https://pypi.python.org/pypi/lda\n",
    "http://www.christianpeccei.com/textmining/\n",
    "http://chrisstrelioff.ws/sandbox/2014/11/13/getting_started_with_latent_dirichlet_allocation_in_python.html\n",
    "\"\"\"\n",
    "\n",
    "import textmining\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "import matplotlib.pyplot as plt\n",
    "import lda\n",
    "\n",
    "word_list = []\n",
    "num_list = []\n",
    "wordcnt = []\n",
    "counter = 1\n",
    "docs = []\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "texts = []\n",
    "# Initialize class to create term-document matrix\n",
    "tdm = textmining.TermDocumentMatrix()\n",
    "# Add the documents\n",
    "\n",
    "with open('short.txt','r') as f:\n",
    "    for line in f:\n",
    "        raw = line.lower()\n",
    "        tokens = raw.split()\n",
    "        for word in tokens:\n",
    "            token = word#.decode(\"ascii\", \"ignore\")\n",
    "            #print token, type(token)\n",
    "            # remove stop words from tokens\n",
    "            if not token in en_stop and token.isalpha():\n",
    "                texts.append(token)\n",
    "        doc = ' '.join(texts)\n",
    "        tdm.add_doc(doc)\n",
    "\n",
    "tdm.write_csv('matrix1.csv', cutoff=1)\n",
    "print (\"Checkpoint 1: CSV File Created\")\n",
    "\"\"\"\n",
    "# Instead of writing out the matrix you can also access its rows directly.\n",
    "# Let's print them to the screen.\n",
    "for row in tdm.rows(cutoff=1):\n",
    "    print row\n",
    "\"\"\"\n",
    "# reading in all data into a NumPy array\n",
    "all_data = np.genfromtxt(\"matrix1.csv\", delimiter=\",\", dtype = None)\n",
    "                      \n",
    "words = all_data[0,:]\n",
    "matrix = all_data[1:,:]\n",
    "print (\"Checkpoint 2: Matrix Data Created\")\n",
    "print (words.shape, matrix.shape)\n",
    "\n",
    "# use matplotlib style sheet\n",
    "try:\n",
    "    plt.style.use('ggplot')\n",
    "except:\n",
    "    # version of matplotlib might not be recent\n",
    "    pass\n",
    "\n",
    "X = matrix.astype(np.int64)\n",
    "vocab = words\n",
    "#titles = lda.datasets.load_reuters_titles()\n",
    "\n",
    "model = lda.LDA(n_topics=20, n_iter=100, random_state=1) #1500\n",
    "model.fit(X)\n",
    "print (\"Checkpoint 3: Model Fitted\")\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "\"\"\" \n",
    "doc_topic = model.doc_topic_\n",
    "for i in range(10):\n",
    "     print('Topic {}: {}'.format(titles[i], doc_topic[i].argmax()))\n",
    "\"\"\"\n",
    "\n",
    "print (\"Checkpoint 4: About to Create Plot\")\n",
    "f, ax= plt.subplots(5, 1, figsize=(8, 6), sharex=True)\n",
    "for i, k in enumerate([0, 5, 9, 14, 19]):\n",
    "    ax[i].stem(topic_word[k,:], linefmt='b-',\n",
    "               markerfmt='bo', basefmt='w-')\n",
    "    ax[i].set_xlim(-50,4350)\n",
    "    ax[i].set_ylim(0, 0.08)\n",
    "    ax[i].set_ylabel(\"Prob\")\n",
    "    ax[i].set_title(\"topic {}\".format(k))\n",
    "\n",
    "ax[4].set_xlabel(\"word\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check\n"
     ]
    }
   ],
   "source": [
    "print (\"Check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Checkpoint 1: CSV File Created. Seconds:', 0.013853788375854492)\n",
      "('Checkpoint 2: Matrix Data Created. Seconds:', 0.037953853607177734)\n",
      "((167,), (11, 167))\n",
      "('Checkpoint 3: Model Fitted. Seconds:', 28.339942932128906)\n",
      "Topic 0: presence elementary multiple paragraph relatively intelligence substantially ability\n",
      "Topic 1: samples developed fall performance difficulties attention ability terms\n",
      "Topic 2: excessively measured standardized decoding ability terms evaluation reading\n",
      "Topic 3: reading disorder age deficit oral interferes feature standardized\n",
      "Topic 4: signs numbers symbols reading written arithmetic understanding linguistic\n",
      "Topic 5: poor measured remediation evaluation grammatical children functional written\n",
      "Topic 6: writing activities disorders oral difficulties ability terms evaluation\n",
      "Topic 7: skills objects may operations including attention remembering daily\n",
      "Topic 8: comprehension activities substantially individuals also speed distortions terms\n",
      "Topic 9: spelling disorder expression known particularly except establish written\n",
      "Topic 10: child organization copy texts written ability terms evaluation\n",
      "Topic 11: written handwriting impairment generally absence early evidenced extent\n",
      "Topic 12: write well combination especially problems errors reading correctly\n",
      "Topic 13: medical achievement oral decoding terms evaluation reading correctly\n",
      "Topic 14: present measured mathematics difficulties education neurological chronological general\n",
      "Topic 15: mathematical ability calculation naming add impaired decoding numerical\n",
      "Topic 16: criterion sensory achievement characterized require tests expected intelligence\n",
      "Topic 17: condition assessment tasks dictation asked case schoolwork oral\n",
      "Topic 18: less diagnosis observing area oral written terms evaluation\n",
      "Topic 19: significantly oral grades sentences necessary within young individually\n",
      "('Checkpoint 4: About to Create Plot. Seconds:', 28.341784954071045)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Dec  9 19:50:08 2015\n",
    "\n",
    "@author: sdeppen\n",
    "\n",
    "sources used\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html#importing-your-documents\n",
    "https://pypi.python.org/pypi/lda\n",
    "http://www.christianpeccei.com/textmining/\n",
    "http://chrisstrelioff.ws/sandbox/2014/11/13/getting_started_with_latent_dirichlet_allocation_in_python.html\n",
    "\"\"\"\n",
    "\n",
    "import textmining\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "import matplotlib.pyplot as plt\n",
    "import lda\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "word_list = []\n",
    "num_list = []\n",
    "wordcnt = []\n",
    "docs = []\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "texts = []\n",
    "# Initialize class to create term-document matrix\n",
    "tdm = textmining.TermDocumentMatrix()\n",
    "# Add the documents\n",
    "\n",
    "with open('short.txt','r') as f:\n",
    "    for line in f:\n",
    "        raw = line.lower()\n",
    "        tokens = raw.split()\n",
    "        for word in tokens:\n",
    "            token = word#.decode(\"ascii\", \"ignore\")\n",
    "            #print token, type(token)\n",
    "            # remove stop words from tokens\n",
    "            if not token in en_stop and token.isalpha():\n",
    "                texts.append(token)\n",
    "        doc = ' '.join(texts)\n",
    "        tdm.add_doc(doc)\n",
    "\n",
    "tdm.write_csv('matrix1.csv', cutoff=1)\n",
    "chk = time.time()\n",
    "cp1 = chk - start_time\n",
    "print (\"Checkpoint 1: CSV File Created. Seconds:\", cp1 )\n",
    "\"\"\"\n",
    "# Instead of writing out the matrix you can also access its rows directly.\n",
    "# Let's print them to the screen.\n",
    "for row in tdm.rows(cutoff=1):\n",
    "    print row\n",
    "\"\"\"\n",
    "# reading in all data into a NumPy array\n",
    "all_data = np.genfromtxt(\"matrix1.csv\", delimiter=\",\", dtype = None)\n",
    "                      \n",
    "words = all_data[0,:]\n",
    "matrix = all_data[1:,:]\n",
    "\n",
    "chk = time.time()\n",
    "cp2 = chk - start_time\n",
    "print (\"Checkpoint 2: Matrix Data Created. Seconds:\", cp2 )\n",
    "print (words.shape, matrix.shape)\n",
    "\n",
    "# use matplotlib style sheet\n",
    "try:\n",
    "    plt.style.use('ggplot')\n",
    "except:\n",
    "    # version of matplotlib might not be recent\n",
    "    pass\n",
    "\n",
    "X = matrix.astype(np.int64)\n",
    "vocab = words\n",
    "#titles = lda.datasets.load_reuters_titles()\n",
    "\n",
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1) #1500\n",
    "model.fit(X)\n",
    "\n",
    "chk = time.time()\n",
    "cp3 = chk - start_time\n",
    "print (\"Checkpoint 3: Model Fitted. Seconds:\", cp3 )\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "\"\"\" \n",
    "doc_topic = model.doc_topic_\n",
    "for i in range(10):\n",
    "     print('Topic {}: {}'.format(titles[i], doc_topic[i].argmax()))\n",
    "\"\"\"\n",
    "\n",
    "chk = time.time()\n",
    "cp4 = chk - start_time\n",
    "print (\"Checkpoint 4: About to Create Plot. Seconds:\", cp4)\n",
    "f, ax= plt.subplots(5, 1, figsize=(8, 6), sharex=True)\n",
    "for i, k in enumerate([0, 5, 9, 14, 19]):\n",
    "    ax[i].stem(topic_word[k,:], linefmt='b-',\n",
    "               markerfmt='bo', basefmt='w-')\n",
    "    ax[i].set_xlim(-50,200) #-50, 4350\n",
    "    ax[i].set_ylim(0, 0.3) #0.08\n",
    "    ax[i].set_ylabel(\"Prob\")\n",
    "    ax[i].set_title(\"topic {}\".format(k))\n",
    "\n",
    "ax[4].set_xlabel(\"word\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Checkpoint 1: CSV File Created. Seconds:', 0.0174410343170166)\n",
      "('Checkpoint 2: Matrix Data Created. Seconds:', 0.031136035919189453)\n",
      "((167,), (11, 167))\n",
      "('Checkpoint 3: Model Fitted. Seconds:', 25.921581029891968)"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Dec  9 19:50:08 2015\n",
    "\n",
    "@author: sdeppen\n",
    "\n",
    "sources used\n",
    "https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html#importing-your-documents\n",
    "https://pypi.python.org/pypi/lda\n",
    "http://www.christianpeccei.com/textmining/\n",
    "http://chrisstrelioff.ws/sandbox/2014/11/13/getting_started_with_latent_dirichlet_allocation_in_python.html\n",
    "\"\"\"\n",
    "\n",
    "import textmining\n",
    "import numpy as np\n",
    "from stop_words import get_stop_words\n",
    "import matplotlib.pyplot as plt\n",
    "import lda\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "word_list = []\n",
    "num_list = []\n",
    "wordcnt = []\n",
    "docs = []\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "texts = []\n",
    "# Initialize class to create term-document matrix\n",
    "tdm = textmining.TermDocumentMatrix()\n",
    "# Add the documents\n",
    "\n",
    "with open('short.txt','r') as f:\n",
    "    for line in f:\n",
    "        raw = line.lower()\n",
    "        tokens = raw.split()\n",
    "        for word in tokens:\n",
    "            token = word#.decode(\"ascii\", \"ignore\")\n",
    "            #print token, type(token)\n",
    "            # remove stop words from tokens\n",
    "            if not token in en_stop and token.isalpha():\n",
    "                texts.append(token)\n",
    "        doc = ' '.join(texts)\n",
    "        tdm.add_doc(doc)\n",
    "\n",
    "tdm.write_csv('matrix1.csv', cutoff=1)\n",
    "chk = time.time()\n",
    "cp1 = chk - start_time\n",
    "print (\"Checkpoint 1: CSV File Created. Seconds:\", cp1 )\n",
    "\"\"\"\n",
    "# Instead of writing out the matrix you can also access its rows directly.\n",
    "# Let's print them to the screen.\n",
    "for row in tdm.rows(cutoff=1):\n",
    "    print row\n",
    "\"\"\"\n",
    "# reading in all data into a NumPy array\n",
    "all_data = np.genfromtxt(\"matrix1.csv\", delimiter=\",\", dtype = None)\n",
    "                      \n",
    "words = all_data[0,:]\n",
    "matrix = all_data[1:,:]\n",
    "\n",
    "chk = time.time()\n",
    "cp2 = chk - start_time\n",
    "print (\"Checkpoint 2: Matrix Data Created. Seconds:\", cp2 )\n",
    "print (words.shape, matrix.shape)\n",
    "\n",
    "# use matplotlib style sheet\n",
    "try:\n",
    "    plt.style.use('ggplot')\n",
    "except:\n",
    "    # version of matplotlib might not be recent\n",
    "    pass\n",
    "\n",
    "X = matrix.astype(np.int64)\n",
    "vocab = words\n",
    "#titles = lda.datasets.load_reuters_titles()\n",
    "\n",
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1) #1500\n",
    "model.fit(X)\n",
    "\n",
    "chk = time.time()\n",
    "cp3 = chk - start_time\n",
    "print (\"Checkpoint 3: Model Fitted. Seconds:\", cp3 )\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "\"\"\" \n",
    "doc_topic = model.doc_topic_\n",
    "for i in range(10):\n",
    "     print('Topic {}: {}'.format(titles[i], doc_topic[i].argmax()))\n",
    "\"\"\"\n",
    "\n",
    "chk = time.time()\n",
    "cp4 = chk - start_time\n",
    "print (\"Checkpoint 4: About to Create Plot. Seconds:\", cp4)\n",
    "f, ax= plt.subplots(5, 1, figsize=(8, 6), sharex=True)\n",
    "for i, k in enumerate([0, 5, 9, 14, 19]):\n",
    "    ax[i].stem(topic_word[k,:], linefmt='b-',\n",
    "               markerfmt='bo', basefmt='w-')\n",
    "    ax[i].set_xlim(-50,200) #-50, 4350\n",
    "    ax[i].set_ylim(0, 0.3) #0.08\n",
    "    ax[i].set_ylabel(\"Prob\")\n",
    "    ax[i].set_title(\"topic {}\".format(k))\n",
    "\n",
    "ax[4].set_xlabel(\"word\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
