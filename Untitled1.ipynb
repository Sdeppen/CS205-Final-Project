{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\"\"\"Latent Dirichlet allocation using collapsed Gibbs sampling\"\"\"\n",
    "\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from __future__ import absolute_import, division, unicode_literals  # noqa\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import lda._lda\n",
    "import lda.utils\n",
    "\n",
    "sc = SparkContext()\n",
    "logger = logging.getLogger('lda')\n",
    "\n",
    "PY2 = sys.version_info[0] == 2\n",
    "if PY2:\n",
    "    range = xrange\n",
    "\n",
    "\n",
    "class LDA:\n",
    "    \"\"\"Latent Dirichlet allocation using collapsed Gibbs sampling\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_topics : int\n",
    "        Number of topics\n",
    "\n",
    "    n_iter : int, default 2000\n",
    "        Number of sampling iterations\n",
    "\n",
    "    alpha : float, default 0.1\n",
    "        Dirichlet parameter for distribution over topics\n",
    "\n",
    "    eta : float, default 0.01\n",
    "        Dirichlet parameter for distribution over words\n",
    "\n",
    "    random_state : int or RandomState, optional\n",
    "        The generator used for the initial topics.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    `components_` : array, shape = [n_topics, n_features]\n",
    "        Point estimate of the topic-word distributions (Phi in literature)\n",
    "    `topic_word_` :\n",
    "        Alias for `components_`\n",
    "    `nzw_` : array, shape = [n_topics, n_features]\n",
    "        Matrix of counts recording topic-word assignments in final iteration.\n",
    "    `ndz_` : array, shape = [n_samples, n_topics]\n",
    "        Matrix of counts recording document-topic assignments in final iteration.\n",
    "    `doc_topic_` : array, shape = [n_samples, n_features]\n",
    "        Point estimate of the document-topic distributions (Theta in literature)\n",
    "    `nz_` : array, shape = [n_topics]\n",
    "        Array of topic assignment counts in final iteration.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy\n",
    "    >>> X = numpy.array([[1,1], [2, 1], [3, 1], [4, 1], [5, 8], [6, 1]])\n",
    "    >>> import lda\n",
    "    >>> model = lda.LDA(n_topics=2, random_state=0, n_iter=100)\n",
    "    >>> model.fit(X) #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    LDA(alpha=...\n",
    "    >>> model.components_\n",
    "    array([[ 0.85714286,  0.14285714],\n",
    "           [ 0.45      ,  0.55      ]])\n",
    "    >>> model.loglikelihood() #doctest: +ELLIPSIS\n",
    "    -40.395...\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent Dirichlet\n",
    "    Allocation.\" Journal of Machine Learning Research 3 (2003): 993–1022.\n",
    "\n",
    "    Griffiths, Thomas L., and Mark Steyvers. \"Finding Scientific Topics.\"\n",
    "    Proceedings of the National Academy of Sciences 101 (2004): 5228–5235.\n",
    "    doi:10.1073/pnas.0307752101.\n",
    "\n",
    "    Wallach, Hanna, David Mimno, and Andrew McCallum. \"Rethinking LDA: Why\n",
    "    Priors Matter.\" In Advances in Neural Information Processing Systems 22,\n",
    "    edited by Y.  Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A.\n",
    "    Culotta, 1973–1981, 2009.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_topics, n_iter=2000, alpha=0.1, eta=0.01, random_state=None,\n",
    "                 refresh=10):\n",
    "        self.n_topics = n_topics\n",
    "        self.n_iter = n_iter\n",
    "        self.alpha = alpha\n",
    "        self.eta = eta\n",
    "        # if random_state is None, check_random_state(None) does nothing\n",
    "        # other than return the current numpy RandomState\n",
    "        self.random_state = random_state\n",
    "        self.refresh = refresh\n",
    "\n",
    "        if alpha <= 0 or eta <= 0:\n",
    "            raise ValueError(\"alpha and eta must be greater than zero\")\n",
    "\n",
    "        # random numbers that are reused\n",
    "        rng = lda.utils.check_random_state(random_state)\n",
    "        self._rands = rng.rand(1024**2 // 8)  # 1MiB of random variates\n",
    "\n",
    "        # configure console logging if not already configured\n",
    "        if len(logger.handlers) == 1 and isinstance(logger.handlers[0], logging.NullHandler):\n",
    "            logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model with X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples in the number of samples\n",
    "            and n_features is the number of features. Sparse matrix allowed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        self._fit(X)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Apply dimensionality reduction on X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            New data, where n_samples in the number of samples\n",
    "            and n_features is the number of features. Sparse matrix allowed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        doc_topic : array-like, shape (n_samples, n_topics)\n",
    "            Point estimate of the document-topic distributions\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(X, np.ndarray):\n",
    "            # in case user passes a (non-sparse) array of shape (n_features,)\n",
    "            # turn it into an array of shape (1, n_features)\n",
    "            X = np.atleast_2d(X)\n",
    "        self._fit(X)\n",
    "        return self.doc_topic_\n",
    "\n",
    "    def transform(self, X, max_iter=20, tol=1e-16):\n",
    "        \"\"\"Transform the data X according to previously fitted model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            New data, where n_samples in the number of samples\n",
    "            and n_features is the number of features.\n",
    "        max_iter : int, optional\n",
    "            Maximum number of iterations in iterated-pseudocount estimation.\n",
    "        tol: double, optional\n",
    "            Tolerance value used in stopping condition.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        doc_topic : array-like, shape (n_samples, n_topics)\n",
    "            Point estimate of the document-topic distributions\n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        This uses the \"iterated pseudo-counts\" approach described\n",
    "        in Wallach et al. (2009) and discussed in Buntine (2009).\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(X, np.ndarray):\n",
    "            # in case user passes a (non-sparse) array of shape (n_features,)\n",
    "            # turn it into an array of shape (1, n_features)\n",
    "            X = np.atleast_2d(X)\n",
    "        doc_topic = np.empty((X.shape[0], self.n_topics))\n",
    "        WS, DS = lda.utils.matrix_to_lists(X)\n",
    "        # TODO: this loop is parallelizable\n",
    "        for d in np.unique(DS):\n",
    "            doc_topic[d] = self._transform_single(WS[DS == d], max_iter, tol)\n",
    "        return doc_topic\n",
    "\n",
    "    def _transform_single(self, doc, max_iter, tol):\n",
    "        \"\"\"Transform a single document according to the previously fit model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1D numpy array of integers\n",
    "            Each element represents a word in the document\n",
    "        max_iter : int\n",
    "            Maximum number of iterations in iterated-pseudocount estimation.\n",
    "        tol: double\n",
    "            Tolerance value used in stopping condition.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        doc_topic : 1D numpy array of length n_topics\n",
    "            Point estimate of the topic distributions for document\n",
    "        \"\"\"\n",
    "        PZS = np.zeros((len(doc), self.n_topics))\n",
    "        for iteration in range(max_iter + 1): # +1 is for initialization\n",
    "            PZS_new = self.components_[:, doc].T\n",
    "            PZS_new *= (PZS.sum(axis=0) - PZS + self.alpha)\n",
    "            PZS_new /= PZS_new.sum(axis=1)[:, np.newaxis] # vector to single column matrix\n",
    "            delta_naive = np.abs(PZS_new - PZS).sum()\n",
    "            logger.debug('transform iter {}, delta {}'.format(iteration, delta_naive))\n",
    "            PZS = PZS_new\n",
    "            if delta_naive < tol:\n",
    "                break\n",
    "        theta_doc = PZS.sum(axis=0) / PZS.sum()\n",
    "        assert len(theta_doc) == self.n_topics\n",
    "        assert theta_doc.shape == (self.n_topics,)\n",
    "        return theta_doc\n",
    "\n",
    "    def _fit(self, X):\n",
    "        \"\"\"Fit the model to the data X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples in the number of samples and\n",
    "            n_features is the number of features. Sparse matrix allowed.\n",
    "        \"\"\"\n",
    "        random_state = lda.utils.check_random_state(self.random_state)\n",
    "        rands = self._rands.copy()\n",
    "        self._initialize(X)\n",
    "        for it in range(self.n_iter):\n",
    "            # FIXME: using numpy.roll with a random shift might be faster\n",
    "            random_state.shuffle(rands)\n",
    "            if it % self.refresh == 0:\n",
    "                ll = self.loglikelihood()\n",
    "                logger.info(\"<{}> log likelihood: {:.0f}\".format(it, ll))\n",
    "                # keep track of loglikelihoods for monitoring convergence\n",
    "                self.loglikelihoods_.append(ll)\n",
    "            sc.parallelize(self._sample_topics(rands), 100)\n",
    "        ll = self.loglikelihood()\n",
    "        logger.info(\"<{}> log likelihood: {:.0f}\".format(self.n_iter - 1, ll))\n",
    "        # note: numpy /= is integer division\n",
    "        self.components_ = (self.nzw_ + self.eta).astype(float)\n",
    "        self.components_ /= np.sum(self.components_, axis=1)[:, np.newaxis]\n",
    "        self.topic_word_ = self.components_\n",
    "        self.doc_topic_ = (self.ndz_ + self.alpha).astype(float)\n",
    "        self.doc_topic_ /= np.sum(self.doc_topic_, axis=1)[:, np.newaxis]\n",
    "\n",
    "        sc.stop()\n",
    "        # delete attributes no longer needed after fitting to save memory and reduce clutter\n",
    "        del self.WS\n",
    "        del self.DS\n",
    "        del self.ZS\n",
    "        return self\n",
    "\n",
    "    def _initialize(self, X):\n",
    "        D, W = X.shape\n",
    "        N = int(X.sum())\n",
    "        n_topics = self.n_topics\n",
    "        n_iter = self.n_iter\n",
    "        logger.info(\"n_documents: {}\".format(D))\n",
    "        logger.info(\"vocab_size: {}\".format(W))\n",
    "        logger.info(\"n_words: {}\".format(N))\n",
    "        logger.info(\"n_topics: {}\".format(n_topics))\n",
    "        logger.info(\"n_iter: {}\".format(n_iter))\n",
    "\n",
    "        self.nzw_ = nzw_ = np.zeros((n_topics, W), dtype=np.intc)\n",
    "        self.ndz_ = ndz_ = np.zeros((D, n_topics), dtype=np.intc)\n",
    "        self.nz_ = nz_ = np.zeros(n_topics, dtype=np.intc)\n",
    "\n",
    "        self.WS, self.DS = WS, DS = lda.utils.matrix_to_lists(X)\n",
    "        self.ZS = ZS = np.empty_like(self.WS, dtype=np.intc)\n",
    "        np.testing.assert_equal(N, len(WS))\n",
    "        for i in range(N):\n",
    "            w, d = WS[i], DS[i]\n",
    "            z_new = i % n_topics\n",
    "            ZS[i] = z_new\n",
    "            ndz_[d, z_new] += 1\n",
    "            nzw_[z_new, w] += 1\n",
    "            nz_[z_new] += 1\n",
    "        self.loglikelihoods_ = []\n",
    "\n",
    "    def loglikelihood(self):\n",
    "        \"\"\"Calculate complete log likelihood, log p(w,z)\n",
    "\n",
    "        Formula used is log p(w,z) = log p(w|z) + log p(z)\n",
    "        \"\"\"\n",
    "        nzw, ndz, nz = self.nzw_, self.ndz_, self.nz_\n",
    "        alpha = self.alpha\n",
    "        eta = self.eta\n",
    "        nd = np.sum(ndz, axis=1).astype(np.intc)\n",
    "        return lda._lda._loglikelihood(nzw, ndz, nz, nd, alpha, eta)\n",
    "\n",
    "    def _sample_topics(self, rands):\n",
    "        \"\"\"Samples all topic assignments. Called once per iteration.\"\"\"\n",
    "        n_topics, vocab_size = self.nzw_.shape\n",
    "        alpha = np.repeat(self.alpha, n_topics).astype(np.float64)\n",
    "        eta = np.repeat(self.eta, vocab_size).astype(np.float64)\n",
    "        lda._lda._sample_topics(self.WS, self.DS, self.ZS, self.nzw_, self.ndz_, self.nz_,\n",
    "                                alpha, eta, rands)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
